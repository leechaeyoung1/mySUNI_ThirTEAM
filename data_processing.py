import pandas as pd
from pathlib import Path
from thefuzz import process, fuzz
from tqdm import tqdm
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import random
import os
from dotenv import load_dotenv
import openai

# ğŸ”‘ OpenAI API í‚¤ (ì„ íƒì ìœ¼ë¡œ ì…ë ¥)
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

VALID_NAMES = ['ê¹€ë¯¼ìˆ˜', 'ë°•ì§€í›ˆ', 'ì´ì§€ìš°', 'ì •í•´ì¸', 'ìµœìœ ë¦¬']

def ask_chatgpt(name: str) -> str:
    if not hasattr(openai, 'api_key') or not openai.api_key:
        return name
    try:
        prompt = (
            f"'{name}'ì´ë¼ëŠ” ì´ë¦„ì— ì˜¤íƒˆìê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            f"ë‹¤ìŒ ëª©ë¡ {VALID_NAMES} ì¤‘ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¦„ì„ ì°¾ì•„ì£¼ì„¸ìš”. "
            f"ìœ ì‚¬í•œ ì´ë¦„ì´ ì—†ë‹¤ë©´ '{name}'ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”. "
            f"ë‹µë³€ì€ ì´ë¦„ í•˜ë‚˜ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤."
        )
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=10,
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return name

def correct_operator(name: str) -> str:
    if not isinstance(name, str) or not name.strip():
        return name
    name = name.strip()
    match, score = process.extractOne(name, VALID_NAMES, scorer=fuzz.ratio)
    if score >= 60:
        return match
    elif score >= 40:
        return ask_chatgpt(name)
    else:
        return name

def refine_status(row):
    remark = str(row.get("remark", ""))
    if any(x in remark for x in ["ì ê²€ì´ í•„ìš”", "ì „ê¸° ê³„í†µì˜ ë¶€í•˜ê°€ ë†’ì•„ì§€ëŠ” ì¶”ì„¸ì„", "ì •ë°€ ì ê²€ì´ í•„ìš”í•¨"]):
        return "ì¶”í›„ ì •ë°€ ì ê²€ í•„ìš”"
    elif any(x in remark for x in ["ê³µì • í’ˆì§ˆì— ì˜í–¥", "ì¦‰ê°ì ì¸ ìœ ì§€ë³´ìˆ˜ê°€ ìš”êµ¬ë¨"]):
        return "ì´ìƒ(êµì²´í•„ìš”)"
    else:
        return "ì´ìƒì—†ìŒ"

def assign_inspector_shift(inspector):
    inspector_shift_map = {
        "ì„œì§€ìš°": "A", "ì´ì¤€ì„œ": "B", "ì •ì¬í›ˆ": "C", "í™ì§€ë¯¼": "A"
    }
    if pd.isna(inspector):
        return random.choice(["A", "B", "C"])
    return inspector_shift_map.get(inspector, random.choice(["A", "B", "C"]))

def safe_merge(df1, df2, on, how="left", name=""):
    keys = [k for k in on if k in df1.columns and k in df2.columns]
    if not keys:
        print(f"âš ï¸ {name} ë³‘í•© ê±´ë„ˆëœ€: ê³µí†µ í‚¤ ì—†ìŒ â†’ {on}")
        return df1
    try:
        return df1.merge(df2, on=keys, how=how)
    except Exception as e:
        print(f"âŒ ë³‘í•© ì‹¤íŒ¨ ({name}):", e)
        return df1

def extract_keywords_tfidf(texts, top_k=4):
    from kiwipiepy import Kiwi
    from sklearn.feature_extraction.text import TfidfVectorizer

    kiwi = Kiwi()
    stopwords = {
        "ì˜", "ì´", "ê°€", "ì€", "ëŠ”", "ë“¤", "ì¢€", "ì˜", "ê±", "ê³¼", "ë„", "ë¥¼", "ìœ¼ë¡œ", "ì—", "í•˜ê³ ", "ë¿", "ë“±",
        "ìˆìœ¼ë©°", "ë˜ì–´", "ìˆ˜", "ìˆë‹¤", "ìˆìŒ", "ë°", "ëŒ€í•œ", "ë•Œë¬¸ì—", "ê²ƒ", "ìˆê³ ", "ìˆì–´"
    }

    def extract_nouns(text):
        return [
            word for word, tag, _, _ in kiwi.analyze(text)[0][0]
            if tag.startswith("NN") and word not in stopwords
        ]

    print(f"ğŸ” í˜•íƒœì†Œ ë¶„ì„ ì¤‘... ì´ {len(texts)}ê°œ ë¬¸ì¥")

    cache = {}
    noun_texts = []
    for i, text in enumerate(texts):
        if i % 500 == 0:
            print(f"  âœ”ï¸ {i}ê°œ ì™„ë£Œ")
        text = text.strip()
        if text in cache:
            nouns = cache[text]
        else:
            try:
                nouns = extract_nouns(text)
            except:
                nouns = []
            cache[text] = nouns
        noun_texts.append(" ".join(nouns))

    vectorizer = TfidfVectorizer(max_features=300)
    X = vectorizer.fit_transform(noun_texts)

    print("ğŸ§  í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
    keywords_list = []
    for i, row in enumerate(X):
        indices = row.nonzero()[1]
        data = row.data
        if len(indices) == 0:
            keywords_list.append("")
            continue
        sorted_indices = sorted(zip(indices, data), key=lambda x: x[1], reverse=True)[:top_k]
        keywords = [vectorizer.get_feature_names_out()[i] for i, _ in sorted_indices]
        keywords_list.append(", ".join(keywords))
        if i % 5000 == 0:
            print(f"ğŸ”¹ í‚¤ì›Œë“œ ì¶”ì¶œ {i}ê°œ ì™„ë£Œ")

    return keywords_list


def run_preprocessing(base_path: Path, openai_key: str = None) -> pd.DataFrame:
    tqdm.pandas()
    if openai_key:
        openai.api_key = openai_key

    # CSV ë¶ˆëŸ¬ì˜¤ê¸°
    production      = pd.read_csv(base_path / "production_log.csv")
    product_master  = pd.read_csv(base_path / "product_master.csv")
    shift_schedule  = pd.read_csv(base_path / "shift_schedule.csv")
    energy_usage    = pd.read_csv(base_path / "energy_usage.csv")
    inspection      = pd.read_csv(base_path / "inspection_result.csv")
    equipment_check = pd.read_csv(base_path / "equipment_check.csv")

    # ë‚ ì§œ ì»¬ëŸ¼ëª… í†µì¼
    production      = production.rename(columns={"production_date": "date"})
    shift_schedule  = shift_schedule.rename(columns={"work_date": "date"})
    inspection      = inspection.rename(columns={"inspection_date": "date"})
    equipment_check = equipment_check.rename(columns={"check_date": "date"})

    for df in [production, shift_schedule, inspection, equipment_check, energy_usage]:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df.dropna(subset=["date"], inplace=True)

    # íƒ€ì… ë³€í™˜
    production["produced_qty"] = pd.to_numeric(production["produced_qty"].replace("ì—†ìŒ", pd.NA), errors="coerce")
    production["defect_qty"] = pd.to_numeric(production["defect_qty"], errors="coerce")

    # í‰ê·  ìƒì‚°ëŸ‰ ë³´ê°„
    avg_map = (
        production[production['defect_qty'].notna()]
        .groupby(['factory_id', 'line_id', 'product_code'])['produced_qty']
        .mean().to_dict()
    )
    def fill_qty(row):
        if pd.isna(row['produced_qty']) and pd.notna(row['defect_qty']):
            key = (row['factory_id'], row['line_id'], row['product_code'])
            return avg_map.get(key, row['produced_qty'])
        return row['produced_qty']
    production["produced_qty"] = production.apply(fill_qty, axis=1)

    # ì˜¤íƒˆì êµì •
    production["operator"] = production["operator"].progress_apply(correct_operator)
    shift_schedule["operator"] = shift_schedule["operator"].progress_apply(correct_operator)

    # equipment_check ìƒíƒœ ì¬ë¶„ë¥˜
    equipment_check["status"] = equipment_check.apply(refine_status, axis=1)

    # ê²€ì‚¬ì› êµëŒ€ì¡°
    inspection["inspector_shift"] = inspection["inspector"].apply(assign_inspector_shift)

    # ë³‘í•©
    merged = production.copy()
    merged = safe_merge(merged, product_master,  on=["product_code"], name="product_master")
    merged = safe_merge(merged, shift_schedule,  on=["factory_id", "line_id", "date", "operator"], name="shift_schedule")
    merged = safe_merge(merged, energy_usage,    on=["factory_id", "line_id", "date"], name="energy_usage")
    merged = safe_merge(merged, inspection,      on=["product_code", "date"], name="inspection")
    merged = safe_merge(merged, equipment_check, on=["factory_id", "line_id", "date"], name="equipment_check")

    # ì»¬ëŸ¼ ì •ë¦¬
    merged["shift"] = merged.groupby(["date", "operator"])["shift"].transform(lambda x: x.ffill().bfill())
    merged["produced_qty"] = merged["produced_qty"].fillna(0).astype(int)
    merged["defect_qty"] = merged["defect_qty"].fillna(0).astype(int)

    # ì»¬ëŸ¼ ì¶”ê°€
    for col in ["equipment_id", "product_name", "category", "spec_weight", "electricity_kwh", "gas_nm3",
                "inspector", "inspector_shift", "result", "status", "remark"]:
        if col not in merged.columns:
            merged[col] = pd.NA

    # í´ëŸ¬ìŠ¤í„°ë§
    tfidf = TfidfVectorizer(max_features=300)
    X = tfidf.fit_transform(merged["remark"].fillna("").astype(str))

    if X.shape[0] < 9:
        raise ValueError("í´ëŸ¬ìŠ¤í„°ë§ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±")

    kmeans = KMeans(n_clusters=9, random_state=42)
    merged["remark_cluster"] = kmeans.fit_predict(X)

    # remark_keywords ìƒì„±
    if "remark" in merged.columns:
        print("ğŸ§  remark_keywords ì¶”ì¶œ ì¤‘...")
        merged["remark"] = merged["remark"].fillna("")
        merged["remark_keywords"] = extract_keywords_tfidf(merged["remark"])

    # ì €ì¥
    BASE = base_path
    merged.to_csv(BASE / "result.csv", index=False, encoding="utf-8-sig")

    return merged
